\documentclass{article}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{float}
\usepackage{url}
\usepackage{hyperref}
\graphicspath{ {Images/} }

\pagestyle{plain}
\setlength{\topmargin}{0.25in}
\setlength{\columnsep}{2.0pc}
\setlength{\headheight}{0.0in}
\setlength{\headsep}{0.0in}
\setlength{\oddsidemargin}{-.19in}
\setlength{\parindent}{1pc}
\textheight 8.75in
\textwidth 6.8in


\begin{document}

\title{\Large \bf Sampling for Join Discovery}

\author{C Rajmohan, Vamshi Pasunuru\\
ME, CSA, IISc Bangalore}
\date{}

\maketitle

\section{Problem Statement}
Given two tables, the join discovery problem is to find out all possible ways in which the two tables can be joined to produce significant results. Each way corresponds to a mapping
of attributes from one table to the other. The strength of a join for a mapping M from table T1 to T2 is the percentage of rows from T1 that join to some row of T2 under the mapping M. We are interested in finding all the mappings that have support greater than a threshold t. This is a very expensive operation since the number of possible mappings is large i.e, NumberOfAttributes(T1) * NumberOfAttributes(T2). In this project, we are using several techniques to prune subset of those mappings. Intuition is that after the first 2 phases of pruning, whatever the mappigs that survived are going to be very small to compared to the original set. Throughout the pruning we could employ sampling technques and use those same results to finally order results based on join support.

\section{Proposed Technique}
Lets say we have two data sets namely Table A(a1,a2,....,am) and Table B(b1,b2,....,bn)
and we want to find potential join attribute pairs between A and B.

Given relation A and B in flat files, our algorithm works in 3 phases.

\subsection{Phase 1}

\begin{itemize}
\item \textbf{Determine datatypes} of each attribute in A and B. This requires one linear scan of the dataset(sampling can be tried out).

\item Eliminate non-compatible attributes by datatypes.
\end{itemize}

\paragraph{Datatype Determination Technique}
Say we want to determine datatype of A.a1 attribute. Candidate datatypes are Integer[1], Numeric[2], String[3] and Date[3]. The numbers in bracket denote rank of the datatype. Ranking is defined to give a priority for data type of an attribute. 

Initially rank of an attribute will be zero. Then we start scanning the tuples values of that attribute. After each scan of a tuple value for a1, rank is updated as follows.

\[rank(a1) = max(rank(a1), rank(CurrentType(a1))\]
We can stop scanning, once rank(a1) reaches 3 without waiting for the complete scan of the dataset.

\subsection{Phase 2}
Out of the remaining combinations, we will use the following techniques to prune some more combinations by taking advantage of specific datatypes.

\paragraph{Integer/Numeric}

Collect the following statistics for both relations by a linear scan(sampling can also be used). 
\begin{itemize}
\item Mean
\item Squared sum
\item Linear sum
\item Variance
\item Covariance 
\item Unique or not
\item Increasing or Decreasing etc.
\end{itemize}


    We can use \textbf{Pearson product-moment correlation coefficient} to study the relationship between the two attributes, if theres a negative coorealtion we can prune it with high confidence.
   Other approcahes are also possible. For example, we can compare statistics of relations A and B. If a pair is not a join pair, it is likely that above parameter values differ significantly. For example, mean value differs by a huge margin when age attribute of table A is compared with zipcode attribute of table B. How much statistics should differ for eliminating a combination needs to be studied further. For instance, we can assign a weight to each of the above parameter and find the weighted sum $W$. If $W$ is above some threshold, it is a potential join pair. Otherwise we can prune it with high confidence. This weighted sum can also be used along with join support to order the results at later stage.
   
\paragraph{String}

Since statistics are not much helpful here, we have come up with the following idea to efficiently prune some of the candidate pairs.

$Sample-JOIN()$ is the traditional join between Table A's a1 attribute and Table B's b1 attribute on a sample of size s.  It returns number of rows from Table A which had a match with some row of Table B.

Parameters to be configured for the algorithm are
\begin{itemize}
\item Threshold t (in percentage). It is the minimum number of rows in A, that join with some row of B.
\item Sample Size s (in percentage)
\item Sampling Repeat Count c. Number of times to repeat sampling
\item Relaxation parameter $\alpha$. It is in (0, 1]. 1/2 is suggested.
\end{itemize}

Take a sample of size s from relations A and B and do a join. Find support. If support is below scaled threshold $st( st = s * t)$, repeat. Sampling can be repeated upto $c$ times.
Compute sum of support. If it is less than $\alpha$ times ideal sum of support$(t * s * size(A) * c)$ prune it.

\paragraph{Heuristics}
\begin{itemize}


\item \textbf{Categorical attributes} Database metadata does not tell us if its string or categorical. we need to write a helper for that. For now lets assume there's one. 	         Since the number of categories are limited, so its not unique which implies that sampling can work well.

\item We expect that $Sample-JOIN(a1,b1)$ will be close to scaled-threshold (which is $t * s$) , to avoid False Negatives we will run this for $c$ times. So $SUM(Sample-JOIN(x,y)) >= (t * s * c * size(A) * alpha)$ should hold. Intuitively it specifies how strict the condition is, if its 1 the algorithm expects that in all runs of sampling we get at-least the scaled-threshold.  The idea is to set it to something like $1/2$ or $3/4$ so that even if in some run the algorithm has less than scaled-threshold it will pass this phase.
\end{itemize}

\subsection{Phase 3}
We muse consolidate the findings of phase 1 and phase 2 and present potential join pairs in support percentage order.
\section{Experiments}

Hive(When partitoned on a column) or PostgreSQL. Has good support for sampling, partitioning. 

In order to evaluate our approach we are comparing our approach with the \textit{oracle's} approach which beforehand knows the support count of all pairs( using full table scan instead of sampling). We are interested in finding the following result.

\begin{itemize}

\item \textbf{How does the sample results correspond to the original results? } How can we ensure that sample has \item \textit{almost} the same probability distribution as that of original data? 

\end{itemize}



\end{document}
