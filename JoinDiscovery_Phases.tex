\documentclass{article}
\usepackage[utf8]{inputenc}

\title{JoinDiscovery}
\author{vamshipasunuru }
\date{February 2015}

\begin{document}

\maketitle

\section{Introduction}
Lets say we have two data sets 
Table A ( a1,a2,....,am)
Table B (b1,b2,....,bn)
and we wanted to find potential join attribute pairs <>




Our algorithm works in 4 phases.

Given: Relation A & B in flat files.

\textbf{
Phase 1
}

Determine datatypes of each attribute in A & B. Do one linear scan for this(sampling can be tried out).

Say we want to determine datatype of A.a1

Potential datatypes: Integer[1], Numeric[2], String[3], Date[3]
The numbers in bracket denote rank of the datatype. Ranking is defined to give a priority for data type of an attribute. 

Initially rank of an attribute will be zero. Then we start scanning the tuples values of that attribute. 

After each scan of a tuple value for a1, rank is updated as follows.

rank(a1) = max(rank(a1), rank(currenttype(a1));

if rank(a1) reaches 3, we stop scanning any further.

At the end of this phase, we know the datatype of each attribute of A & B. Eliminate non-compatible attributes by datatypes.

\textbf{phase 2}
Now out of the remaining combinations, we will use the following techniques to prune some more combinations. It takes advantage of the datatype.

\textbf{Integer/Numeric}
There are two possible techniques that can be employed here.

1. Collect the following statistics for both relations by linear scan(sampling can also be used). 
                     => Mean
                     => Squared sum
                     => Linear sum
                     => Variance
                     => Unique /Non unique
                     => Increasing /Decreasing etc.

   Compare statistics of relations A & B. If a pair is not join attributes, it is likely that above parameter values differ significantly. For example, age of table1 when joined with zipcode of table2, mean differs by a huge margin. How much the difference in statistics should be, for eliminating a combination needs to be studied further. For instance, we can assign a weight to each of the above parameter and find the weighted sum W. If W is above some threshold, it is a potential join pair. Otherwise we can prune it with high confidence. This weighted sum can be used along with join support to order the results.


String :  


Since statistics are nt of much help here, we have come up with the following idea to efficiently prune some of the candidates.

Parameters to be configured for the algorithm

Threshold t (in percentage)
 - Minimum number of rows in A, that join with some row of B.
 
Sample Size s (in percentage)

Sampling Repeat Count c
- Number of times to repeat sampling

Relaxation parameter alpha
- in (0, 1]. 1/2 is suggested.

Sample-JOIN() is the traditional join between Table A's x attribute and Table B's y attribute on a sample of size s.  It returns number of rows from Table A which had a match with the some row of Table B.

Take a sample of size s from relations A & B and do a join. Find support. If support is below scaled threshold st( st = t * s), repeat. Sampling can be repeated upto c times.

Compute sum of support. If it is less than alpha times ideal sum of support(t * s * size(A) * c) prune it.

heuristics: 

For Categorical attributes:
	         First of all, db metadata does not tell us if its string or categorical. we need to write a helper for that. For now lets assume there's one.
	         Since the number of categories are limited, so its not unique which implies that sampling can work well.
	         we expect that Sample-JOIN(x,y) is close to scaled-threshold (which is t * s) , to avoid False Negatives we will run this for "c" times, 
	         So SUM(Sample-JOIN(x,y)) >= (t * s * c * size(A) * alpha) should hold. Intuitively it specifies how strict the condition is, if its 1 the algorithm expects that in all runs of sampling we get at-least the scaled-threshold.  The idea is to set it to something like 1/2 or 3/4 so that even if in some run the algorithm has less than scaled-threshold it will pass this phase.

Implementation ideas:

1. Hive & Hadoop or postgreSQL. nice support for sampling, partitioning. 






\end{document}
